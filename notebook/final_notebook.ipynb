{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":107555,"databundleVersionId":13033998,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## POKEMON COMPETITION","metadata":{}},{"cell_type":"markdown","source":"## Repository cloning\nAll our auxiliary code (feature engineering, utility functions, and model definitions) is stored in a separate GitHub repository.\nWe clone the repository directly into the Kaggle environment so that the notebook can import and use the modules inside the src/ folder.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/maf1013/pokemonprediction-fds.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:46:55.662934Z","iopub.execute_input":"2025-11-13T19:46:55.663440Z","iopub.status.idle":"2025-11-13T19:46:56.273761Z","shell.execute_reply.started":"2025-11-13T19:46:55.663401Z","shell.execute_reply":"2025-11-13T19:46:56.272448Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'pokemonprediction-fds'...\nremote: Enumerating objects: 18, done.\u001b[K\nremote: Counting objects: 100% (18/18), done.\u001b[K\nremote: Compressing objects: 100% (11/11), done.\u001b[K\nremote: Total 18 (delta 4), reused 18 (delta 4), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (18/18), 17.37 KiB | 889.00 KiB/s, done.\nResolving deltas: 100% (4/4), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Add path to `sys.path`\n\nAfter cloning the repository, we add its folder to `sys.path` so that Python can find the modules in `src` and import them without errors.\n","metadata":{}},{"cell_type":"code","source":"import sys\n\nrepo_root = \"/kaggle/working/pokemonprediction-fds\"\nsrc_path  = \"/kaggle/working/pokemonprediction-fds/src\"\n\n\nfor p in [repo_root, src_path]:\n    if p not in sys.path:\n        sys.path.append(p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:49:05.363192Z","iopub.execute_input":"2025-11-13T19:49:05.363567Z","iopub.status.idle":"2025-11-13T19:49:05.369293Z","shell.execute_reply.started":"2025-11-13T19:49:05.363541Z","shell.execute_reply":"2025-11-13T19:49:05.368268Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Features (`src/features.py`)\n\nThis project uses a custom module `features.py` to transform each battle into a set of numerical variables that the models can learn from. The main functions are:\n\n#### learn_type_effectiveness()\n\nAutomatically learns a type effectiveness table using only the real outcomes from the dataset.  \nFor each pair of types:\n\n- Counts how many times they faced each other.  \n- Calculates how many times each one won.  \n- If there is little data → it is considered neutral (1.0).  \n- If there is enough data → computes a smoothed effectiveness (alpha, beta).  \n- Limits the value between 0.75 and 1.25 to avoid extremes.  \n\nThe result is a matrix that indicates which type tends to win against which in the dataset, later used for features such as `type_advantage`.\n\n#### summarize_timeline()\n\nSummarizes the entire sequence of the battle:\n\n- Number of turns  \n- Pokémon switches  \n- Status changes  \n- Damage received per turn  \n- HP advantage per player  \n- Momentum and control shifts  \n- Whether there was a comeback  \n\nIt converts the battle’s progression into numerical variables that help the model understand how the battle was won.\n\n#### team_static_features()\n\nExtracts team statistics for each player:\n\n- Means, maxima, minima, and standard deviations of base stats  \n- Number of distinct team types  \n- Type entropy (diversity)  \n\nReturns all these features with a `p1_` or `p2_` prefix.\n\n#### create_features_from_battle()\n\nCombines all components:\n\n- Team stats  \n- Type advantages (player vs opponent)  \n- Attribute differences  \n- Battle timeline summary  \n- Initial and final advantage  \n\nProduces a dictionary with all the features of a battle.\n\n#### build_feature_df()\n\nApplies `create_features_from_battle()` to each battle in the dataset and builds the final DataFrame used to train the models.\n","metadata":{}},{"cell_type":"code","source":"from model_random import run_random\nfrom model_stacking import run_stacking\nfrom model_voting import run_voting\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:49:27.623842Z","iopub.execute_input":"2025-11-13T19:49:27.625564Z","iopub.status.idle":"2025-11-13T19:49:27.630593Z","shell.execute_reply.started":"2025-11-13T19:49:27.625517Z","shell.execute_reply":"2025-11-13T19:49:27.629474Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Model `Random Forest`\nAfter generating all the features and removing constant columns, we train a Random Forest Classifier, a model based on many decision trees that vote together.\n\nWe use stratified cross-validation to evaluate its performance in a stable way and avoid overfitting.\nFinally, we train the full model with all available data and predict the outcomes of the battles in the test set.\n\nThe goal of this model is to provide a solid baseline against which the more advanced models (Voting and Stacking) can be compared.","metadata":{}},{"cell_type":"code","source":"run_random()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:49:30.413235Z","iopub.execute_input":"2025-11-13T19:49:30.413647Z","iopub.status.idle":"2025-11-13T19:50:24.659847Z","shell.execute_reply.started":"2025-11-13T19:49:30.413620Z","shell.execute_reply":"2025-11-13T19:50:24.658881Z"}},"outputs":[{"name":"stdout","text":"Running Random model...\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 10000/10000 [00:07<00:00, 1387.88it/s]\nFeatures: 100%|██████████| 5000/5000 [00:03<00:00, 1421.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Removing constant columns: ['diff_base_hp', 'diff_base_atk', 'diff_base_def', 'diff_base_spa', 'diff_base_spd', 'diff_base_spe', 'n_turns']\n\nFold 1\nFold 1 accuracy: 0.8125\n\nFold 2\nFold 2 accuracy: 0.8185\n\nFold 3\nFold 3 accuracy: 0.8175\n\nFold 4\nFold 4 accuracy: 0.8085\n\nFold 5\nFold 5 accuracy: 0.8125\n\nOOF Accuracy: 0.8139\n\nCSV generated: submission_random.csv\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Model `Voting`\n\nThis model combines two different algorithms — XGBoost and AdaBoost — using soft voting.\nEach model computes its probability of winning, and the VotingClassifier produces a final prediction by averaging both.\n\nThe idea is that XGBoost provides the ability to model complex interactions, while AdaBoost offers robustness and simplicity.\nBy combining them, the ensemble tends to be more stable and generalizes better than either model on its own.\n\nThis model also uses cross-validation to measure its performance, and then it is trained on all the data to generate the final prediction.","metadata":{}},{"cell_type":"code","source":"run_voting()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:51:27.379354Z","iopub.execute_input":"2025-11-13T19:51:27.379800Z","iopub.status.idle":"2025-11-13T19:55:23.746976Z","shell.execute_reply.started":"2025-11-13T19:51:27.379771Z","shell.execute_reply":"2025-11-13T19:55:23.745947Z"}},"outputs":[{"name":"stdout","text":"Loading data\nTrain: 10000 | Test: 5000\n\nCross validation (VotingClassifier: XGB + AdaBoost)...\n\nFold 1\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 8000/8000 [00:05<00:00, 1439.37it/s]\nFeatures: 100%|██████████| 2000/2000 [00:01<00:00, 1466.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 accuracy: 0.8190\n\nFold 2\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 8000/8000 [00:05<00:00, 1466.29it/s]\nFeatures: 100%|██████████| 2000/2000 [00:01<00:00, 1459.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold 2 accuracy: 0.8285\n\nFold 3\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 8000/8000 [00:05<00:00, 1433.18it/s]\nFeatures: 100%|██████████| 2000/2000 [00:01<00:00, 1423.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold 3 accuracy: 0.8295\n\nFold 4\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 8000/8000 [00:05<00:00, 1425.03it/s]\nFeatures: 100%|██████████| 2000/2000 [00:01<00:00, 1397.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold 4 accuracy: 0.8220\n\nFold 5\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 8000/8000 [00:05<00:00, 1461.73it/s]\nFeatures: 100%|██████████| 2000/2000 [00:01<00:00, 1387.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Fold 5 accuracy: 0.8255\n\nTraining the final model and generating the submission...\n","output_type":"stream"},{"name":"stderr","text":"Features: 100%|██████████| 10000/10000 [00:06<00:00, 1439.70it/s]\nFeatures: 100%|██████████| 5000/5000 [00:03<00:00, 1436.55it/s]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### Model `Stacking`\n\nThis model combines three different algorithms (XGBoost, AdaBoost, and Gradient Boosting) using stacking.\nInstead of voting, each model generates its predictions, and a final meta-model (Logistic Regression) learns how to combine them to improve accuracy.\n\nEach model contributes a different perspective:\n\nThe system is trained with cross-validation to ensure robustness and is then fitted on all the data before generating the final prediction.","metadata":{}},{"cell_type":"code","source":"run_stacking()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}